{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "#copyfile('../back_last_time.hdf5' , '../every.best.hdf5')\n",
    "#copyfile('../every.best.hdf5', '../back_last_time.91.33.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# path to the model weights files.\n",
    "top_model_weights_path = '../vgg16_weights.h5'\n",
    "filepath='../every.best.hdf5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 243, 243\n",
    "\n",
    "train_data_dir = '../dataset/training_set/smo'\n",
    "validation_data_dir = '../dataset/test_set/smo'\n",
    "\n",
    "nb_train_samples = 5106\n",
    "nb_validation_samples = 1355\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the VGG16 network\n",
    "\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "print('base_model loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "#top_model.load_weights(weights_path)\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "\n",
    "#top_model.load_weights(top_model_weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "# model.add(top_model)\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "#model.load_weights('../every.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "\n",
    "#for layer in model.layers[:15]:    #if using 25, it mean it will not train anything!\n",
    "    #layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5106 images belonging to 2 classes.\n",
      "Found 1355 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "train_history=model.fit_generator(\n",
    "    train_generator,\n",
    "       ###steps_per_epoch=nb_train_samples  // batch_size,  \n",
    "    steps_per_epoch=nb_train_samples, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "       ###validation_steps=nb_validation_samples  // batch_size,\n",
    "    validation_steps=nb_validation_samples,\n",
    "    callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('../every.best.hdf5', '../vgg16.0.96089.hdf5')\n",
    "score = model.evaluate_generator(validation_generator, nb_validation_samples/batch_size, workers=12)\n",
    "\n",
    "scores = model.predict_generator(validation_generator, nb_validation_samples/batch_size , workers=12)\n",
    "correct = 0\n",
    "for i, n in enumerate(validation_generator.filenames):\n",
    "    if n.startswith(\"cats\") and scores[i][0] <= 0.5:\n",
    "        correct += 1\n",
    "    if n.startswith(\"dogs\") and scores[i][0] > 0.5:\n",
    "        correct += 1\n",
    "\n",
    "print(\"Correct:\", correct, \" Total: \", len(validation_generator.filenames))\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing.image import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "#model = load_model('../vgg16.95351.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_acc,test_acc):\n",
    "    plt.plot(train_history.history[train_acc])\n",
    "    plt.plot(train_history.history[test_acc])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "#model.save_weights(top_model_weights_path)\n",
    "#model.save_weights('../every.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history('acc','val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('../dataset/single_prediction/mama.jpg', target_size= (img_width, img_height))\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "inputarray = test_image[np.newaxis,...] # dimension added to fit input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =  model.predict(inputarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "print(\"Predicted=%s\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_image_expand_dims = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image_expand_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### new #####################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from IPython.display import display, Image\n",
    "\n",
    "################################################################################    \n",
    "# Import the required modules\n",
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Grab the input device, in this case the webcam\n",
    "# You can also give path to the video file\n",
    "vid = cv2.VideoCapture(\"../dataset/source/smorking.mp4\")\n",
    "\n",
    "# Put the code in try-except statements\n",
    "# Catch the keyboard exception and \n",
    "# release the camera device and \n",
    "# continue with the rest of code.\n",
    "#plt.figure(figsize = (10,10))\n",
    "#fig = plt.figure()\n",
    "#print (\"fig = \") \n",
    "#<Figure size 432x288 with 0 Axes>\n",
    "label_no_smorking = \"predict: no Smorking\"\n",
    "label_smorking = \"predict: Smorking!\"\n",
    "count = 0\n",
    "\n",
    "#'''\n",
    "try:\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            # Release the Video Device if ret is false\n",
    "            vid.release()\n",
    "            # Message to be displayed after releasing the device\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "            \n",
    "        # Write image to folder\n",
    "        #img_jpg = np.asarray(frame)\n",
    "        #cv2.imwrite(\"../dataset/source/out/cyril_frame%d.jpg\" % count, img_jpg)\n",
    "        #count = count + 1\n",
    "        \n",
    "        #predict image here !\n",
    "        resize = cv2.resize(frame, (img_width, img_height)) \n",
    "        resize = np.asarray(resize)\n",
    "        test_image = image.img_to_array(resize)\n",
    "        test_image_expand_dims = np.expand_dims(test_image, axis = 0)\n",
    "        result = model.predict(test_image_expand_dims)\n",
    "        #ans = result[0][1]  //why VGG16 is fail wtih #ans = result[0][1] \n",
    "        #print(result)\n",
    "        ans = result\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "         \n",
    "        if ans > 0.45: \n",
    "        \n",
    "            # put text into image \n",
    "            # https://blog.csdn.net/GAN_player/article/details/78155283\n",
    "            cv2.putText(frame, label_smorking, (30, 80),  cv2.FONT_HERSHEY_SIMPLEX,1.0, (255,0, 0), 2)\n",
    "            #cv2.line(影像, 開始座標, 結束座標, 顏色, 線條寬度)\n",
    "            cv2.line(frame, (0, 0), (432, 0),     (255,0,0),  12)\n",
    "            cv2.line(frame, (0, 0), (0, 350),     (255,0,0),  12)\n",
    "            cv2.line(frame, (440, 0),(440, 440),  (255,0,0),  12)\n",
    "            cv2.line(frame, (0, 330),(440, 330),(255,0,0),12)\n",
    "\n",
    "        else:\n",
    "             # put text into image \n",
    "            # https://blog.csdn.net/GAN_player/article/details/78155283\n",
    "            cv2.putText(frame, label_no_smorking, (30, 80),  cv2.FONT_HERSHEY_SIMPLEX,1.0, (255,255, 255), 2)\n",
    "            #cv2.line(影像, 開始座標, 結束座標, 顏色, 線條寬度)\n",
    "          \n",
    "        # Turn off the axis\n",
    "        plt.axis('off')\n",
    "        # Title of the window\n",
    "        plt.title(\"Detecting ..\")\n",
    "        # Display the frame\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        # Display the frame until new frame is available\n",
    "        clear_output(wait=True)\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    #print(\"Released Video Resource\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from scipy import misc\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from keras.preprocessing import image\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from IPython.display import display, Image\n",
    "\n",
    "\n",
    "video_data_dir = '../dataset/source/smorking.mp4'\n",
    "#-------------------------------------------------------------------\n",
    "vidcap = cv2.VideoCapture(video_data_dir)\n",
    "#-------------------------------------------------------------------\n",
    "video_fps= int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "print (\"CAP_PROP_FPS= \",video_fps)\n",
    "#-------------------------------------------------------------------\n",
    "#video_pos = int(vidcap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "#print (\"CAP_PROP_POS_MSEC= \",video_pos)\n",
    "#-------------------------------------------------------------------\n",
    "video_w = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print (\"CAP_PROP_FRAME_WIDTH= \",video_w)\n",
    "#-------------------------------------------------------------------\n",
    "video_h = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print (\"CAP_PROP_FRAME_HEIGHT= \",video_h)\n",
    "#-------------------------------------------------------------------\n",
    "video_total_frame = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print (\"video_total_frame (CAP_PROP_FRAME_COUNT) = \",video_total_frame)\n",
    "#-------------------------------------------------------------------\n",
    "video_total_tims=  video_total_frame / video_fps\n",
    "print (\"video_total_frame = \",video_total_tims)\n",
    "video_total_tims_msec = video_total_tims * 1000\n",
    "print (\"video_total_tims_msec = \",video_total_tims_msec)\n",
    "#-------------------------------------------------------------------\n",
    "#video_total_times = int(vidcap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "#print (\"CV_CAP_PROP_POS_MSEC = \",video_total_times)\n",
    "\n",
    "\n",
    "msec_count = 0\n",
    "\n",
    "while msec_count <= video_total_tims_msec:\n",
    "\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC, msec_count) \n",
    "    success,imagecv2 = vidcap.read()\n",
    "    resize = cv2.resize(imagecv2, (img_width, img_height)) \n",
    "    test_image = image.img_to_array(resize)\n",
    "    test_image_expand_dims = np.expand_dims(test_image, axis = 0)\n",
    "    result = model.predict(test_image_expand_dims)\n",
    "    #ans = result[0][1]\n",
    "    ans = result\n",
    "\n",
    "    #print(ans)\n",
    "    \n",
    "    #save image here\n",
    "    #img_jpg = np.asarray(resize)    \n",
    "    #cv2.imwrite(\"../dataset/tmp/frame%d.jpg\" % msec_count, img_jpg)\n",
    "  \n",
    "    if ans > 0.45 :\n",
    "        \n",
    "        #convert RGB of image for IPL\n",
    "        frame = cv2.cvtColor(resize, cv2.COLOR_BGR2RGB) ### for IPL.show!!!\n",
    "\n",
    "        print (\"smorking---------------\")\n",
    "        #plt.imshow(img)\n",
    "        im = PIL.Image.fromarray(frame)\n",
    "        bio = BytesIO()\n",
    "        im.save(bio, format='png')\n",
    "        display(Image(bio.getvalue(), format='png'))        \n",
    "        \n",
    "    else :\n",
    "        '''\n",
    "        print (\"no smorking here ---------------\")\n",
    "\n",
    "        #convert RGB of image for IPL\n",
    "        frame = cv2.cvtColor(resize, cv2.COLOR_BGR2RGB) ### for IPL.show!!!\n",
    "\n",
    "        print (\"no smorking---------------\")\n",
    "        #plt.imshow(img)\n",
    "        im = PIL.Image.fromarray(frame)\n",
    "        bio = BytesIO()\n",
    "        im.save(bio, format='png')\n",
    "        display(Image(bio.getvalue(), format='png'))  \n",
    "        \n",
    "        print (\"-------------------------------------------------\")\n",
    "        '''\n",
    "    msec_count = msec_count + (2000)\n",
    "\n",
    "\n",
    "print ( \"done, counts are\", msec_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
