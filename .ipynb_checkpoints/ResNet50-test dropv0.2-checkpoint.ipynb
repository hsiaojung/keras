{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aewin/work/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing.image import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(2017)\n",
    "\n",
    "from skimage.io import imread_collection, imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "filepath='../every.resn50.hdf5'\n",
    "img_width, img_height = 243, 243\n",
    "\n",
    "train_data_dir = '../dataset/training_set/smo'\n",
    "validation_data_dir = '../dataset/test_set/smo'\n",
    "\n",
    "nb_train_samples = 4521\n",
    "nb_validation_samples = 770\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', pooling='avg'\n",
    "                       ,input_tensor=Input((img_width, img_height, 3)))\n",
    "    # -  https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
    "    # -  https://keras.io/zh/layers/core/\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    #x = Flatten()(base_model.output)#将输入展平。不影响批量大小。\n",
    "    x = Dropout(0.2)(base_model.output) # this line shoule  be the first line!\n",
    "    x = Dense(2, activation='softmax', name='fc2')(base_model.output) #加入神經元(隱藏層) 2,foftmax\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "  \n",
    "    for layer in model.layers[140:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "\n",
    "    train_path ='../dataset/training_set/smo'\n",
    "    test_path = '../dataset/test_set/smo'\n",
    "\n",
    "    ''' \n",
    "    Optimizers which is simple yet very efficient approach to discriminative learning \n",
    "    of linear classifiers under convex loss functions such as (linear) Support\n",
    "    Vector Machines and Logistic Regression\n",
    "    #### some type optomizer:#   https://keras.io/optimizers/#adadelta\n",
    "    '''\n",
    "    #optiz = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.1)\n",
    "    optiz = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0) #https://keras.io/optimizers/#adadelta\n",
    "    \n",
    "    model.compile(optimizer=optiz,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # # X_train, y_train, X_val, y_val = train_test_split(X, y, test_size=0.1)\n",
    "    # model.fit(X, y, batch_size=32, epochs=15, validation_split=0.1)\n",
    "\n",
    "    data_gen = ImageDataGenerator()\n",
    "    \n",
    "    train_data_gen = data_gen.flow_from_directory(train_path, (img_width, img_height), shuffle='False', batch_size=32)\n",
    "    val_data_gen = data_gen.flow_from_directory(test_path, (img_width, img_height), shuffle='False', batch_size=32)\n",
    "    \n",
    "    model.summary()\n",
    "    #model = load_model('../fine_tuning_dogcat_resnet50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4521/4521 [==============================] - 607s 134ms/step - loss: 5.8147e-04 - acc: 0.9999 - val_loss: 0.9103 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.88831, saving model to ../every.resn50.hdf5\n",
      "Epoch 2/20\n",
      "4521/4521 [==============================] - 626s 138ms/step - loss: 5.1788e-06 - acc: 1.0000 - val_loss: 0.9604 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.88831\n",
      "Epoch 3/20\n",
      "4521/4521 [==============================] - 620s 137ms/step - loss: 3.7793e-06 - acc: 1.0000 - val_loss: 0.9966 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.88831 to 0.88961, saving model to ../every.resn50.hdf5\n",
      "Epoch 4/20\n",
      "4521/4521 [==============================] - 617s 137ms/step - loss: 4.6928e-06 - acc: 1.0000 - val_loss: 0.9654 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88961\n",
      "Epoch 5/20\n",
      "4521/4521 [==============================] - 606s 134ms/step - loss: 1.3072e-04 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88961\n",
      "Epoch 6/20\n",
      "4521/4521 [==============================] - 614s 136ms/step - loss: 1.9086e-06 - acc: 1.0000 - val_loss: 1.0809 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88961\n",
      "Epoch 7/20\n",
      "4521/4521 [==============================] - 621s 137ms/step - loss: 4.0054e-06 - acc: 1.0000 - val_loss: 1.0963 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88961\n",
      "Epoch 8/20\n",
      "4521/4521 [==============================] - 626s 138ms/step - loss: 1.7944e-05 - acc: 1.0000 - val_loss: 0.9933 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88961\n",
      "Epoch 9/20\n",
      "4521/4521 [==============================] - 621s 137ms/step - loss: 4.5390e-05 - acc: 1.0000 - val_loss: 0.8169 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.88961 to 0.90130, saving model to ../every.resn50.hdf5\n",
      "Epoch 10/20\n",
      "4521/4521 [==============================] - 629s 139ms/step - loss: 7.5134e-05 - acc: 1.0000 - val_loss: 0.9590 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90130\n",
      "Epoch 11/20\n",
      "4521/4521 [==============================] - 624s 138ms/step - loss: 1.3490e-06 - acc: 1.0000 - val_loss: 0.9661 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90130\n",
      "Epoch 12/20\n",
      "1222/4521 [=======>......................] - ETA: 7:32 - loss: 5.8518e-07 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-555cb44d12ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_validation_samples\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#model.save('fine_tuning_dogcat_resnet502')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "model.fit_generator(train_data_gen, steps_per_epoch=nb_train_samples, epochs=epochs, validation_data=val_data_gen, validation_steps=nb_validation_samples,  callbacks=callbacks_list)\n",
    "\n",
    "#model.save('fine_tuning_dogcat_resnet502')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eb6f00575ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_train_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_train_history' is not defined"
     ]
    }
   ],
   "source": [
    "show_train_history('acc','val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('../resnet50.89481')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "  \n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from IPython.display import display, Image\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_path = '../dataset/source/out/'\n",
    "    # [cat, dog]\n",
    "    # yiyi is 1 if the image is a dog, 0 if cat\n",
    "\n",
    "    #model = load_model('/home/zhangzhe/pycharm/dogcat/src/fine_tuning_dogcat_resnet50')\n",
    "    #print('model load finished')\n",
    "\n",
    "    files = os.listdir(test_path)\n",
    "\n",
    "    prediction = []\n",
    "    file_indexs = []\n",
    "    im_ind = 0\n",
    "    print('Start Prediction: ')\n",
    "    for file in files:\n",
    "        print(im_ind, file)\n",
    "        img_path = os.path.join(test_path, file)\n",
    "        img_pil = load_img(img_path, target_size=(img_width, img_height))\n",
    "        img = img_to_array(img_pil)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        result = model.predict(img)\n",
    "        file_name, suffix = file.split('.')\n",
    "        prediction.append(result[0][1])\n",
    "        file_indexs.append(file_name)\n",
    "        ans = result[0][1];\n",
    "        print('show ans= ', ans)\n",
    "       \n",
    "        if ans > 0.45:\n",
    "            \n",
    "            print (\"smorking----------smorking------smorking------------------\")\n",
    "            img = cv2.imread(img_path)\n",
    "            #cv2.imshow(winname='show the image',mat=img) \n",
    "            im = PIL.Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))  #OpenCV转换成PIL.Image格式\n",
    "            #plt.imshow(im)\n",
    "            bio = BytesIO()\n",
    "            im.save(bio, format='png')\n",
    "            display(Image(bio.getvalue(), format='jpg'))\n",
    "            print (\"-------------------------------------------------\")\n",
    "            \n",
    "        else:\n",
    "            '''\n",
    "            print (\"no no no smorking------------------------\")\n",
    "            img = cv2.imread(img_path)\n",
    "            #cv2.imshow(winname='show the image',mat=img) \n",
    "            im = PIL.Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))  #OpenCV转换成PIL.Image格式\n",
    "            #plt.imshow(im)\n",
    "            bio = BytesIO()\n",
    "            im.save(bio, format='png')\n",
    "            display(Image(bio.getvalue(), format='jpg'))\n",
    "            print (\"-------------------------------------------------\")\n",
    "            '''\n",
    "        print('     ', result[0][1], file_name)\n",
    "        im_ind += 1\n",
    "        \n",
    "    print('Finish prediction. ')\n",
    "\n",
    "    #df = pd.DataFrame({'id':file_indexs, 'label':prediction})\n",
    "    #df.to_csv('pred2.csv', index=None)\n",
    "    #df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing.image import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "img_width, img_height = 243, 243\n",
    "\n",
    "\n",
    "\n",
    "model = load_model('../fine_tuning_dogcat_resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    while(True):\\n        # Capture frame-by-frame\\n        ret, frame = vidcap.read()\\n        \\n        \\n        \\n        if not ret:\\n            # Release the Video Device if ret is false\\n            vidcap.release()\\n            # Message to be displayed after releasing the device\\n            print(\"Released Video Resource\")\\n            break\\n        # Convert the image from OpenCV BGR format to matplotlib RGB format\\n        # to display the image\\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n        #frame = cv2.namedWindow(\"enhanced\",0);\\n        #frame = cv2.resizeWindow(frame, 320, 240)\\n\\n        # Turn off the axis\\n        axis(\\'off\\')\\n        # Title of the window\\n        title(\"Input Stream\")\\n        # Display the frame\\n        imshow(frame)\\n        show()\\n        # Display the frame until new frame is available\\n        clear_output(wait=True)\\n        \\n        \\nexcept KeyboardInterrupt:\\n    # Release the Video Device\\n    vidcap.release()\\n    # Message to be displayed after releasing the device\\n    #print(\"Released Video Resource\")\\n  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "  \n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from IPython.display import display, Image\n",
    "\n",
    "\n",
    "\n",
    "video_data_dir = '../dataset/source/smorking.mp4'\n",
    "\n",
    "vidcap = cv2.VideoCapture(video_data_dir)\n",
    "\n",
    "\n",
    "video_fps= int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "#print (\"CAP_PROP_FPS= \",video_fps)\n",
    "\n",
    "video_pos = int(vidcap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "#print (\"CAP_PROP_POS_MSEC= \",video_pos)\n",
    "\n",
    "video_w = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#print (\"CAP_PROP_FRAME_WIDTH= \",video_w)\n",
    "\n",
    "video_h = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#print (\"CAP_PROP_FRAME_HEIGHT= \",video_h)\n",
    "\n",
    "\n",
    "video_l = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#print (\"CAP_PROP_FRAME_COUNT = \",video_l)\n",
    "\n",
    "# Read until video is completed\n",
    "while(vidcap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = vidcap.read()\n",
    "  if ret == True:\n",
    "    # Display the resulting frame\n",
    "    #cv2.imshow('Frame',frame)\n",
    "    plt.imshow(frame)\n",
    "    # Press Q on keyboard to  exit\n",
    "    #if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      #break\n",
    " \n",
    "  # Break the loop\n",
    "  else: \n",
    "    break    \n",
    "'''\n",
    "try:\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = vidcap.read()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if not ret:\n",
    "            # Release the Video Device if ret is false\n",
    "            vidcap.release()\n",
    "            # Message to be displayed after releasing the device\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #frame = cv2.namedWindow(\"enhanced\",0);\n",
    "        #frame = cv2.resizeWindow(frame, 320, 240)\n",
    "\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(frame)\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    vidcap.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    #print(\"Released Video Resource\")\n",
    "  '''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
